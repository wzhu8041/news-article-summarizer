{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unzip File from Model Notebook"
      ],
      "metadata": {
        "id": "0SzVmHOJ8BMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q bart_summarizer_finetuned.zip -d ./bart_summarizer_finetuned\n"
      ],
      "metadata": {
        "id": "GBqQRomBub5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Packages"
      ],
      "metadata": {
        "id": "815voez88LOU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUTjVg_g3t_s"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U transformers accelerate torch\n",
        "!pip -q install -U newspaper3k lxml_html_clean\n",
        "!pip -q install ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import newspaper3k and widgets"
      ],
      "metadata": {
        "id": "ODTocG2_8QFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import newspaper\n",
        "from newspaper import Article\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "esa-BJWX31W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Model from Directory"
      ],
      "metadata": {
        "id": "gYWdfgMz8ZeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
        "save_dir = PROJECT_ROOT / \"models\" / \"bart_summarizer_finetuned\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(save_dir).to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Loaded on:\", device)"
      ],
      "metadata": {
        "id": "f13oDrwmHnWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetching the Title and Contents of Article from Link"
      ],
      "metadata": {
        "id": "0VXB9M1r8drT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_article(url: str, language: str = \"en\", char_cap: int = 30000):\n",
        "    art = Article(url, language=language)\n",
        "    art.download()\n",
        "    art.parse()\n",
        "    text = (art.text or \"\").strip()\n",
        "\n",
        "    # cap to avoid super long pages (Yahoo can be huge)\n",
        "    if char_cap is not None and len(text) > char_cap:\n",
        "        text = text[:char_cap]\n",
        "\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"title\": (art.title or \"\").strip(),\n",
        "        \"text\": text\n",
        "    }\n"
      ],
      "metadata": {
        "id": "KOAN1kZQHvg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarize text using the Pretrained Model"
      ],
      "metadata": {
        "id": "Bkdj7cRQ8qQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_texts(\n",
        "    texts,\n",
        "    max_new_tokens: int = 200,\n",
        "    min_new_tokens: int = 40,\n",
        "    num_beams: int = 4,\n",
        "):\n",
        "  with torch.no_grad():\n",
        "    cleaned = [t if (t and t.strip()) else \"\" for t in texts]\n",
        "\n",
        "    max_input_len = getattr(model.config, \"max_position_embeddings\", 1024)  # BART is typically 1024\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        cleaned,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_input_len,\n",
        "    ).to(device)\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        num_beams=num_beams,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        min_new_tokens=min_new_tokens,\n",
        "        length_penalty=1.0,\n",
        "        no_repeat_ngram_size=3,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "\n",
        "    return [tokenizer.decode(ids, skip_special_tokens=True).strip() for ids in output_ids]\n"
      ],
      "metadata": {
        "id": "M27c2kKQ3iM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarizing the Article from a Link"
      ],
      "metadata": {
        "id": "HtNo-RJa80Bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import List\n",
        "def summarize_links(\n",
        "    urls,\n",
        "    language: str = \"en\",\n",
        "    batch_size: int = 4,   # increase if GPU can handle it\n",
        "):\n",
        "    \"\"\"\n",
        "    Variable number of links -> one summary per link.\n",
        "    \"\"\"\n",
        "    # 1) scrape\n",
        "    articles = [fetch_article(u, language=language) for u in urls]\n",
        "\n",
        "    # 2) summarize in batches\n",
        "    summaries = []\n",
        "    texts = [a[\"text\"] for a in articles]\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        batch_summaries = summarize_texts(batch)\n",
        "        summaries.extend(batch_summaries)\n",
        "\n",
        "    # 3) pack results\n",
        "    results = []\n",
        "    for art, summ in zip(articles, summaries):\n",
        "        results.append({\n",
        "            \"url\": art[\"url\"],\n",
        "            \"title\": art[\"title\"],\n",
        "            \"chars_extracted\": len(art[\"text\"]),\n",
        "            \"summary\": summ if art[\"text\"] else \"Could not extract article text (site may block scraping).\"\n",
        "        })\n",
        "    return results"
      ],
      "metadata": {
        "id": "FkuZ1lDJH1Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Widget Interface to paste links"
      ],
      "metadata": {
        "id": "csweVqL09c2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Articles to use:\n",
        "https://finance.yahoo.com/news/why-shares-wix-com-stock-204914840.html\n",
        "\n",
        "https://finance.yahoo.com/news/exxon-mobil-xom-forecasts-lower-205226995.html\n",
        "\n",
        "https://finance.yahoo.com/news/chevron-cvx-talks-expand-oil-205238211.html"
      ],
      "metadata": {
        "id": "njOB_VAV9Gb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "MAX_LINKS = 3\n",
        "\n",
        "boxes = [widgets.Text(\n",
        "    placeholder=f\"Paste link {i+1} (optional)\",\n",
        "    description=f\"URL {i+1}:\",\n",
        "    layout=widgets.Layout(width=\"90%\")\n",
        ") for i in range(MAX_LINKS)]\n",
        "\n",
        "btn = widgets.Button(description=\"Summarize\", button_style=\"primary\")\n",
        "out = widgets.Output()\n",
        "\n",
        "def on_click(_):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        urls = [b.value.strip() for b in boxes if b.value.strip()]\n",
        "        if not urls:\n",
        "            print(\"Please enter at least one URL.\")\n",
        "            return\n",
        "\n",
        "        results = summarize_links(urls, batch_size=1)  # your existing function\n",
        "\n",
        "        for i, r in enumerate(results, 1):\n",
        "            print(f\"\\n--- Article {i} ---\")\n",
        "            print(\"Title:\", r[\"title\"])\n",
        "            print(\"URL:\", r[\"url\"])\n",
        "            print(\"Extracted chars:\", r[\"chars_extracted\"])\n",
        "            print(\"Summary:\", r[\"summary\"])\n",
        "\n",
        "btn.on_click(on_click)\n",
        "\n",
        "display(*boxes, btn, out)\n"
      ],
      "metadata": {
        "id": "WlAA0fsl36aT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}